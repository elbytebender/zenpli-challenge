{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zenpli challenge\n",
        "\n",
        "### Parte 1.2\n"
      ],
      "metadata": {
        "id": "8bjSqO8k-Dat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar Java, Spark, and Findspark\n",
        "El siguiente bloque de codigo instala Apache Spark 3.4.0, Java 8 y [Findspark](https://github.com/minrk/findspark), una librería que hace facil instalar Spark en ambientes express (Google Colab, etc)"
      ],
      "metadata": {
        "id": "qPx5KJyS-OvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xze3LYDJ-Cr7"
      },
      "outputs": [],
      "source": [
        "!rm -rf spark-3.4.0-bin-hadoop3.tgz\n",
        "!rm -rf spark-3.4.0-bin-hadoop3\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Varuiables de entorno para findspark"
      ],
      "metadata": {
        "id": "TWRPiKb9-vSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "os.environ[\"SPARK_VERSION\"] = \"3.4\""
      ],
      "metadata": {
        "id": "VLFug4z_-ylK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n"
      ],
      "metadata": {
        "id": "mZiKyZPQ-3uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "pCtcHkkY-4jM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "from pyspark.sql.types import (ArrayType, LongType, StringType, StructField, StructType, IntegerType, StringType, DateType, DoubleType, TimestampType)\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "bzNrPO07_Adt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "akBZDCuX_ME9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la configuración de Spark y creamos la sesión"
      ],
      "metadata": {
        "id": "uvvcJ8WX_M86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf() \\\n",
        "    .setMaster('local[*]') \\\n",
        "    .set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\") \\\n",
        "    .set(\"spark.sql.parquet.mergeSchema\", \"true\") \\\n",
        "    .set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\") \\\n",
        "    .set(\"spark.sql.caseSensitive\", \"true\") \\\n",
        "    .set(\"spark.storage.memoryFraction\", 1) \\\n",
        "    .set(\"spark.executor.memory\", \"20g\") \\\n",
        "    .set(\"spark.driver.memory\", \"20g\") \\\n",
        "    .set(\"spark.cores.max\", 5) \\\n",
        "    .set(\"spark.executor.cores\", 5)"
      ],
      "metadata": {
        "id": "riBqSc0h_NG0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Local\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"zenpli-challenge-app\") \\\n",
        "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
        "    .config(conf=conf) \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Zfaz5ZiN_PH9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el esquema apropiado"
      ],
      "metadata": {
        "id": "wGp34iI0_bSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"key_1\", StringType()),\n",
        "    StructField(\"date_2\", TimestampType()),\n",
        "    StructField(\"cont_3\", DoubleType()),\n",
        "    StructField(\"cont_4\", DoubleType()),\n",
        "    StructField(\"disc_5\", IntegerType()),\n",
        "    StructField(\"disc_6\", IntegerType()),\n",
        "    StructField(\"cat_7\", StringType()),\n",
        "    StructField(\"cat_8\", StringType()),\n",
        "    StructField(\"cont_9\", DoubleType()),\n",
        "    StructField(\"cont_10\", DoubleType())\n",
        "])"
      ],
      "metadata": {
        "id": "UbvAsEIq_bv0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la variable con la ruta apuntando al archivo local"
      ],
      "metadata": {
        "id": "sQME3NuzCj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'file:///content/backend-dev-data-dataset.txt'"
      ],
      "metadata": {
        "id": "nb2RcEsLCf-6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos la lectura"
      ],
      "metadata": {
        "id": "zpprgdeOCxNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "        .schema(schema) \\\n",
        "        .option(\"header\",True) \\\n",
        "        .option(\"delimiter\",\",\") \\\n",
        "        .option(\"quote\", \"\\\"\") \\\n",
        "        .option(\"escape\", \"\\\"\") \\\n",
        "        .option(\"unescapedQuoteHandling\", \"STOP_AT_DELIMITER\") \\\n",
        "        .csv(data_path)"
      ],
      "metadata": {
        "id": "xfLW_qVB_eus"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos la cantidad de registros"
      ],
      "metadata": {
        "id": "LSoGda9ACwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ_cZnO9_Wtq",
        "outputId": "28877379-da9e-4f0e-c3e9-327ccd254ad9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "572364"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos esquema"
      ],
      "metadata": {
        "id": "4v4T4MTwC3Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQWJ5yqP_W2S",
        "outputId": "5bf46d1b-dcbf-472e-fae3-183492edc970"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- key_1: string (nullable = true)\n",
            " |-- date_2: timestamp (nullable = true)\n",
            " |-- cont_3: double (nullable = true)\n",
            " |-- cont_4: double (nullable = true)\n",
            " |-- disc_5: integer (nullable = true)\n",
            " |-- disc_6: integer (nullable = true)\n",
            " |-- cat_7: string (nullable = true)\n",
            " |-- cat_8: string (nullable = true)\n",
            " |-- cont_9: double (nullable = true)\n",
            " |-- cont_10: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos los primeros 10 registros"
      ],
      "metadata": {
        "id": "fGRkzCpzC5q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2COTteD_DPE8",
        "outputId": "aca2684d-c1e9-44eb-fbd0-a584d1bbfa0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
            "| key_1|             date_2| cont_3|cont_4|disc_5|disc_6|   cat_7|    cat_8|cont_9|cont_10|\n",
            "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
            "|HC2030|2016-11-16 00:00:00| 622.27| -2.36|     2|     6|frequent|    happy|  0.24|   0.25|\n",
            "|sP8147|2004-02-18 00:00:00|1056.16| 59.93|     2|     8|   never|    happy|  1.94|   2.29|\n",
            "|Cq3823|2007-03-25 00:00:00| 210.73|-93.94|     1|     1|   never|    happy| -0.11|   -0.1|\n",
            "|Hw9428|2013-12-28 00:00:00|1116.48| 80.58|     3|    10|   never|surprised|  1.27|   1.15|\n",
            "|xZ0360|2003-08-25 00:00:00| 1038.3| 12.37|     6|    17|   never|    happy|  1.76|   1.76|\n",
            "|IK2721|2012-10-19 00:00:00| 835.17|  16.3|     4|    11|frequent|surprised|  2.04|    2.3|\n",
            "|iK8875|2005-02-04 00:00:00| 769.02| 75.69|     3|     2|   never|    happy| -1.53|  -1.56|\n",
            "|qd0312|2014-11-17 00:00:00| 273.11|  66.2|     1|     8|frequent|surprised|  2.67|   2.95|\n",
            "|IO1104|2020-11-24 00:00:00|1844.21|-54.11|     1|    11|   never|surprised| -0.42|  -0.43|\n",
            "|mb3668|2002-02-26 00:00:00|2369.77|165.12|     2|     7|   never|    happy| -1.11|  -1.15|\n",
            "+------+-------------------+-------+------+------+------+--------+---------+------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Normalizar una columna (cualquiera de valores continuos);\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tLyO0HC0_qU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierta los valores de la columna en un vector denso requerido para MinMaxScaler\n",
        "assembler = VectorAssembler(inputCols=[\"cont_3\"], outputCol=\"cont_3_vector\")\n",
        "dfn = assembler.transform(df)\n",
        "\n",
        "# Inicializamos el MinMaxScaler\n",
        "scaler = MinMaxScaler(inputCol=\"cont_3_vector\", outputCol=\"cont_3_norm\")\n",
        "\n",
        "# Fit + transform para normalizar la columna en el dataframe\n",
        "scaler_model = scaler.fit(dfn)\n",
        "df_normalized = scaler_model.transform(dfn)\n",
        "\n",
        "# Tomo las columnas relevantes\n",
        "df_normalized = df_normalized.select(\"key_1\", \"cont_3\",\"cont_3_norm\").orderBy(F.col(\"cont_3\"))\n",
        "\n",
        "min_value = dfn.select(F.min(\"cont_3\")).first()[0]\n",
        "max_value = dfn.select(F.max(\"cont_3\")).first()[0]\n",
        "\n",
        "# Obtengo el min y max de la columna normalizada\n",
        "min_scaled_value = df_normalized.select(F.min(\"cont_3_norm\")).first()[0]\n",
        "max_scaled_value = df_normalized.select(F.max(\"cont_3_norm\")).first()[0]\n",
        "\n",
        "# Show the results\n",
        "print(f\"Rangos originales de la columna 'cont_3': [{min_value}, {max_value}]\")\n",
        "print(f\"Rangos normalizados en la nueva columna 'cont_3_norm': [{min_scaled_value}, {max_scaled_value}]\")\n",
        "\n",
        "# Ver resultados\n",
        "df_normalized.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajc9TwsG_sKl",
        "outputId": "f7abbeb8-0488-4c44-94ee-7924560368a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rangos originales de la columna 'cont_3': [8.78, 127527.83]\n",
            "Rangos normalizados en la nueva columna 'cont_3_norm': [[0.0], [1.0]]\n",
            "+------+------+--------------------+\n",
            "| key_1|cont_3|         cont_3_norm|\n",
            "+------+------+--------------------+\n",
            "|Vg2186|  8.78|               [0.0]|\n",
            "|Fr0592| 13.91|[4.02292833894229...|\n",
            "|CT5216| 14.01|[4.10134799467216...|\n",
            "|PI4573| 14.27|[4.30523909956982...|\n",
            "|tf6780| 14.31|[4.33660696186177...|\n",
            "|af8802|  16.2|[5.81873845515630...|\n",
            "|xl1206| 16.23|[5.84226435187526...|\n",
            "|tm2866| 16.27|[5.87363221416721...|\n",
            "|nf1924| 16.27|[5.87363221416721...|\n",
            "|ib3166| 16.27|[5.87363221416721...|\n",
            "|nr4259|  16.6|[6.13241707807578...|\n",
            "|xg4886| 16.99|[6.43825373542227...|\n",
            "|tL2721| 17.64|[6.94798149766642...|\n",
            "|vR6289| 17.99|[7.22245029272096...|\n",
            "|Ov1552| 18.03|[7.25381815501292...|\n",
            "|uH1020| 18.27|[7.44202532876460...|\n",
            "|YL9808| 18.45|[7.58318070907836...|\n",
            "|Nf8845| 18.71|[7.78707181397603...|\n",
            "|Xr6024| 18.78|[7.84196557298693...|\n",
            "|WC0621| 18.85|[7.89685933199784...|\n",
            "+------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Filtrar una columna por cierto valor (cualquiera de valores categóricos);"
      ],
      "metadata": {
        "id": "GnDD0uk_BamE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_8_with_most_records = df.groupBy(\"cat_8\").agg(F.count(F.col(\"key_1\")).alias(\"cnt_keys\")).orderBy(F.col(\"cnt_keys\").desc()).first()[0]\n",
        "\n",
        "df_filtered_range = df.filter((F.col(\"cat_8\") == cat_8_with_most_records))\n",
        "df_filtered_range.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgbuR5nHBa1t",
        "outputId": "d5ca433f-601b-4699-c938-9616a012abf9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+-------+-------+------+------+--------+-----+------+-------+\n",
            "| key_1|             date_2| cont_3| cont_4|disc_5|disc_6|   cat_7|cat_8|cont_9|cont_10|\n",
            "+------+-------------------+-------+-------+------+------+--------+-----+------+-------+\n",
            "|HC2030|2016-11-16 00:00:00| 622.27|  -2.36|     2|     6|frequent|happy|  0.24|   0.25|\n",
            "|sP8147|2004-02-18 00:00:00|1056.16|  59.93|     2|     8|   never|happy|  1.94|   2.29|\n",
            "|Cq3823|2007-03-25 00:00:00| 210.73| -93.94|     1|     1|   never|happy| -0.11|   -0.1|\n",
            "|xZ0360|2003-08-25 00:00:00| 1038.3|  12.37|     6|    17|   never|happy|  1.76|   1.76|\n",
            "|iK8875|2005-02-04 00:00:00| 769.02|  75.69|     3|     2|   never|happy| -1.53|  -1.56|\n",
            "|mb3668|2002-02-26 00:00:00|2369.77| 165.12|     2|     7|   never|happy| -1.11|  -1.15|\n",
            "|Ch8767|2019-06-26 00:00:00| 259.95| -21.14|     4|     7|   never|happy|  0.06|   0.05|\n",
            "|MB6485|2009-08-05 00:00:00|2432.88|  -9.38|     1|     9|frequent|happy| -1.15|   null|\n",
            "|ja8141|2019-11-16 00:00:00|  55.65|  17.73|     5|     9|   never|happy| -1.77|  -1.49|\n",
            "|eX8597|2004-06-27 00:00:00|  462.2| -15.62|     6|    12|frequent|happy|  0.25|   0.24|\n",
            "|JW7796|2013-02-15 00:00:00| 610.23|  -49.3|     3|     8|frequent|happy|  0.76|   0.67|\n",
            "|sc4013|2017-08-30 00:00:00| 825.26| -29.39|     5|    11|frequent|happy|  0.26|   0.22|\n",
            "|os0804|2003-06-30 00:00:00| 105.92|   5.41|     4|    16|frequent|happy| -0.39|  -0.38|\n",
            "|iE5884|2014-10-20 00:00:00| 316.07|  11.84|     5|     4|frequent|happy| -2.13|  -2.28|\n",
            "|SR7714|2016-11-07 00:00:00| 205.65| -48.76|     3|    16|frequent|happy|  -0.5|   -0.6|\n",
            "|Ab1168|2007-03-20 00:00:00|1529.84|-131.83|     2|     4|   never|happy| -0.66|   -0.7|\n",
            "|yU9569|2004-02-08 00:00:00|3295.74| -12.61|     2|    14|frequent|happy|  1.37|   1.45|\n",
            "|EH0448|2011-12-26 00:00:00| 404.29| 116.97|     4|  null|   never|happy|  -0.8|  -0.83|\n",
            "|dR5330|2011-08-01 00:00:00| 941.19|-204.28|     5|     8|frequent|happy|  0.63|   0.63|\n",
            "|hf4768|2010-03-25 00:00:00|  131.7| -65.43|     5|     1|frequent|happy|  0.35|   0.33|\n",
            "+------+-------------------+-------+-------+------+------+--------+-----+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.   Agrupar ciertas columnas (cualesquiera que correspondan a fechas)."
      ],
      "metadata": {
        "id": "MdmmTolcBhQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"data\")\n",
        "\n",
        "date_df = spark.sql(\"\"\"\n",
        "  select date(date_2) as date_2,\n",
        "         round(avg(cont_3),3) as avg_cont_3,\n",
        "         sum(disc_5) as sum_disc_5,\n",
        "         count(cat_7) as cnt_cat_7,\n",
        "         count(distinct cat_7) as cntd_cat_7\n",
        "  from data\n",
        "  group by 1\n",
        "  order by 1 asc\n",
        "\"\"\")\n",
        "\n",
        "date_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8_0s-90BkAs",
        "outputId": "effd6ec5-dd4f-4c23-c2bf-50e609c6c6d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+---------+----------+\n",
            "|    date_2|avg_cont_3|sum_disc_5|cnt_cat_7|cntd_cat_7|\n",
            "+----------+----------+----------+---------+----------+\n",
            "|2000-01-01|  1307.283|       261|       80|         4|\n",
            "|2000-01-02|  1566.393|       278|       71|         3|\n",
            "|2000-01-03|  1633.249|       207|       60|         4|\n",
            "|2000-01-04|  1714.456|       308|       80|         4|\n",
            "|2000-01-05|  1906.198|       231|       64|         4|\n",
            "|2000-01-06|  1222.739|       244|       72|         4|\n",
            "|2000-01-07|  1795.397|       271|       79|         4|\n",
            "|2000-01-08|  1630.553|       272|       80|         4|\n",
            "|2000-01-09|  1519.671|       273|       76|         3|\n",
            "|2000-01-10|  1825.371|       284|       74|         4|\n",
            "|2000-01-11|  1663.886|       271|       80|         4|\n",
            "|2000-01-12|   1457.76|       231|       63|         3|\n",
            "|2000-01-13|  1553.177|       277|       78|         3|\n",
            "|2000-01-14|   1743.32|       253|       68|         4|\n",
            "|2000-01-15|  1332.923|       254|       81|         4|\n",
            "|2000-01-16|  1925.264|       259|       73|         4|\n",
            "|2000-01-17|  1918.956|       233|       75|         3|\n",
            "|2000-01-18|  1725.188|       255|       73|         3|\n",
            "|2000-01-19|  1561.034|       273|       78|         3|\n",
            "|2000-01-20|  1582.353|       311|       84|         4|\n",
            "+----------+----------+----------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}